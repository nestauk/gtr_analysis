{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inline matplotlib output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Setup\n",
    "\n",
    "As we are accessing data on a remote PostgreSQL server, we need to create a Psycopg2 `connection` object using our database parameters. These are read in from a JSON file at the path below. We use these to construct a connection string which is passed as the only parameter to the connect method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../scripts/config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4737dbb9ac83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read in config file with DB params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../scripts/config.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define a connection string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../scripts/config.json'"
     ]
    }
   ],
   "source": [
    "# Read in config file with DB params\n",
    "with open('../../scripts/config.json') as f:\n",
    "    conf = json.load(f)\n",
    "    \n",
    "# Define a connection string\n",
    "conn_string = 'host={} dbname={} user={} password={}'.format(conf.get('host'),\n",
    "                                                             conf.get('database'),\n",
    "                                                             conf.get('user'),\n",
    "                                                             conf.get('passw'))\n",
    "\n",
    "# Create a connection object\n",
    "conn = psycopg2.connect(conn_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persons\n",
    "\n",
    "This section of the notebook creates a DataFrame object holding data relating to persons in the Gateway to Research database. We first read in the data from a PostgreSQL database, expanding nested JSON elements to individual rows using Postgres' [jsonb_array_elements](http://www.postgresql.org/docs/9.4/static/functions-json.html) function.\n",
    "\n",
    "As this isn't a small database we use the pandas.read_sql `chunksize` parameter, which returns an iterator when specified. Instead of reading in every line of the data returned from a sql query, this iterator reads in `chunk` number of lines at a time, removing the requirement to hold every row in-memory while creating python objects to represent the data (which typically take up far less memory).\n",
    "\n",
    "The difference between this approach and the usual flow of directly creating a DataFrame is that we define the iterator `results` and an empty DataFrame `df` to which we append data `chunk` number of times while looping over the iterator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of rows to iterate at a time\n",
    "chunk = 5000\n",
    "\n",
    "# SQL string that unpacks nested JSON arrays\n",
    "sql_str = \"\"\"\n",
    "SELECT \n",
    "  id, first_name, surname, \n",
    "  jsonb_array_elements(links->'link')->'rel' as relationship,\n",
    "  jsonb_array_elements(links->'link')->'href' AS href,\n",
    "  jsonb_array_elements(links->'link')->'otherAttributes' as other_attribs\n",
    "FROM\n",
    "  gtr.persons;\n",
    "\"\"\"\n",
    "\n",
    "# chunksize returns an iterator that reads chunk number of rows at a time\n",
    "results = pd.read_sql(sql_str,\n",
    "                      conn,\n",
    "                      chunksize=chunk)\n",
    "\n",
    "# New dataframe object that can be appended to\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Iterate through result\n",
    "# This can take a while on large tables\n",
    "for result in results:\n",
    "    df = df.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the `df` object was an empty DataFrame that has now been appended to, the index will conatin duplicate values in the range 0 - (`chunk` - 1), so we reset the index to get unique integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that index values are reoccuring\n",
    "df.index.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove non-unique integers in index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.index.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check everything looks normal\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the aspects of these data we are interested in are the links between researchers via projects they have worked on. The SQL query pulled through the URIs for projects that people worked on and their organisations (`href` column) as well as their relationship to that project or organisation (`relationship`).\n",
    "\n",
    "To make the URI easier to match to the project/organisation ID later on we split it and keep just the string after the last forward slash. This corresponds to the project/organisation ID which is contained in the project database. This is easily achieved using the `map` method and a `lambda` expression with a list slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We only want the unique identifier from the href\n",
    "df.href = df.href.map(lambda x: x.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at some basic stats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of researchers:\\t\\t{}\".format(df.id.nunique()))\n",
    "print(\"Number of projects:\\t\\t{}\".format(df[df.relationship != \"EMPLOYED\"].href.nunique()))\n",
    "print(\"Number of organisations:\\t{}\".format(df[df.relationship == \"EMPLOYED\"].href.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at some stats across researchers, it's easier to group them by their id values, carry out the analyses, and then recombine the data. We do this by creating a GroupedBy object using the `groupby` DataFrame method, passing it the name of the column we wish to group by as a string.\n",
    "\n",
    "The `len` built-in function returns the number of keys in the GroupedBy object dictionary representing the groups. We can use this to check the number of groups is the same as the number of researchers. We will create two GroupedBy objecs, one with just projects and one with only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grp_projects = df[df.relationship != 'EMPLOYED'].groupby('id')\n",
    "grp_orgs = df[df.relationship == 'EMPLOYED'].groupby('id')\n",
    "print('Project groups: {}\\nOrganisation groups: {}'.format(len(grp_projects), len(grp_orgs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all researchers have an organisation link, but not all have a project link. Lets look at the data on number of projects by researcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_projects = grp_projects['href'].count().mean()\n",
    "max_projects = grp_projects['href'].count().max()\n",
    "print(\"Mean number of projects per researcher: {}\".format(avg_projects))\n",
    "print(\"Max number of projects per researcher: {}\".format(max_projects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5,348 projects for a single person doesn't sound sensible. Lets look at the count of person's project links in descending order to see whether there are other issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grp_projects['href'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this that there are a small number of `persons` that have a very high number of projects. It makes sense to check their details to see whether we want to remove them from the analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the ids as a list\n",
    "ids = [x for x in grp_projects['href'].count().sort_values(ascending=False)[0:3].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[(df['id'].isin(ids)) & (df['relationship'] != 'EMPLOYED')].groupby('id').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this that those persons with a high number of project links actually all have `PM_PER` links (which from the GtR [online dictionary](http://gtr.rcuk.ac.uk/resources/GtRDataDictionary.pdf) we can see are *project managers* and not researchers). Lets check what other `relationship` types there are, mapping them back to the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.relationship.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that there are a number of researcher person types:\n",
    "- COI_PER (Co-investigator)\n",
    "- PI_PER (Principal investigator)\n",
    "- RESEARCH_COI_PER (Post-doc research assistant with COI status)\n",
    "- FELLOW_PER (Research Fellow)\n",
    "- RESEARCH_PER (Post-doc research assistant)\n",
    "\n",
    "As well as non-researcher person types:\n",
    "- EMPLOYED (Employing organisation)\n",
    "- TGH_PER (Training Grant Holder)\n",
    "- PM_PER (Project manager)\n",
    "\n",
    "The 'SUPER_PER' maps to supervisors of doctoral training students.\n",
    "\n",
    "So we need to be smarter in our GroupBy method calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grp_projects = df[(df.relationship != 'EMPLOYED')\n",
    "                  & (df.relationship != 'PM_PER')\n",
    "                  & (df.relationship != 'SUPER_PER')\n",
    "                  & (df.relationship != 'TGH_PER')].groupby('id')\n",
    "\n",
    "grp_orgs = df[df.relationship == 'EMPLOYED'].groupby('id')\n",
    "print('Project groups: {}\\nOrganisation groups: {}'.format(len(grp_projects), len(grp_orgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_projects = grp_projects['href'].count().mean()\n",
    "max_projects = grp_projects['href'].count().max()\n",
    "min_projects = grp_projects['href'].count().min()\n",
    "print(\"Mean number of projects per researcher: {:.2}\".format(avg_projects))\n",
    "print(\"Max number of projects per researcher: {}\".format(max_projects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all we want to look at for now in terms of the Persons data. We'll move on to the Projects data next, which will be similar in terms of method execution, so I'll be less verbose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projects\n",
    "\n",
    "We take a slightly different approach to reading projects data as these are stored as materialized views in PostgreSQL. To read them in, we get seperate dataframes and then use `pd.merge` using the `id` as a key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_project_data(type):\n",
    "    sql_str = \"SELECT * FROM gtr.projects_{};\".format(type)\n",
    "    \n",
    "    # chunksize returns an iterator that reads chunk number of rows at a time\n",
    "    results = pd.read_sql(sql_str,\n",
    "                      conn,\n",
    "                      #parse_dates=['created'],\n",
    "                      chunksize=100000)\n",
    "    \n",
    "    dfx = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through result\n",
    "    # This can take a while on large tables\n",
    "    for result in results:\n",
    "        dfx = dfx.append(result)\n",
    "\n",
    "    # Appending creates non-unique integers in index\n",
    "    dfx.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return dfx.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_projects_subject = get_project_data('subject')\n",
    "df_projects_subject.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_projects_topic = get_project_data('topic')\n",
    "df_projects_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_projects_link = get_project_data('link')\n",
    "df_projects_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql_str = \"SELECT id, grant_cats, lead_org_dpts, status, titles as title FROM gtr.projects;\"\n",
    "    \n",
    "# chunksize returns an iterator that reads chunk number of rows at a time\n",
    "results = pd.read_sql(sql_str,\n",
    "                  conn,\n",
    "                  #parse_dates=['created'],\n",
    "                  chunksize=100000)\n",
    "\n",
    "df_projects = pd.DataFrame()\n",
    "\n",
    "# Iterate through result\n",
    "# This can take a while on large tables\n",
    "for result in results:\n",
    "    df_projects = df_projects.append(result)\n",
    "\n",
    "# Appending creates non-unique integers in index\n",
    "df_projects.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_projects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge all the dataframes using the ID as key\n",
    "df_merged = df_projects.merge(\n",
    "                df_projects_topic, on='id', sort=False, how='left').merge(\n",
    "                    df_projects_link, on='id', sort=False, how='left').merge(\n",
    "                        df_projects_subject, on='id', sort=False, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sense check\n",
    "df_merged.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcomes\n",
    "\n",
    "We can look at the `rel` column to identify the number and types of outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rel unique values\n",
    "df_merged.rel.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a list of relevant outcomes and filter on that to view outcomes that are relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outcomes = ['IP',\n",
    "            'PRODUCT',\n",
    "            'RESEARCH_DATABASE_AND_MODEL',\n",
    "            'POLICY',\n",
    "            'ARTISTIC_AND_CREATIVE_PRODUCT',\n",
    "            'SOFTWARE_AND_TECHNICAL_PRODUCT',\n",
    "            'SPIN_OUT']\n",
    "\n",
    "df_merged[df_merged.rel.isin(outcomes)].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[print('{}: {}'.format(outcome, df_merged[df_merged == outcome].rel.count())) for outcome in outcomes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_merged[(df_merged.id == 'B1EE98F8-4E51-40B7-8C3D-A882E08BC321')].reset_index().ix[0].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_merged[(df_merged.id == 'B1EE98F8-4E51-40B7-8C3D-A882E08BC321') &\n",
    "          (df_merged.rel.isin(outcomes))].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 65,053 projects, only 805 have at least one outcome recorded against them. The project with the single largest number of *relevant* outcomes against it is *IMPRINTS Identity Management: Public Responses to IdeNtity Technologies and Services*, at 275 outcomes. Of these, 150 were POLICY outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the total number of all relevant outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_merged.rel[df_merged.rel.isin(outcomes)].value_counts().sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
